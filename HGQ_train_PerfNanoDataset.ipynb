{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de45e21-4ed0-4c5e-8466-97b52ea4f39d",
   "metadata": {},
   "source": [
    "## HGQ train of models: MLP , DeepSets , MLPMixer , JEDILinear and Lipschitz MLPs on PerNano dataset\n",
    "Based on Cheng Su implementations using HGQ \n",
    "https://github.com/calad0i/JEDI-linear/blob/master/src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d21e6b-2848-4b12-963e-b172c21630f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "keras backend: tensorflow\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# Standard library\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import threading\n",
    "import pickle as pkl\n",
    "from math import log2, cos, pi\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Third-party scientific / data\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import h5py as h5\n",
    "import psutil\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# TensorFlow / Keras\n",
    "# (Use ONLY tensorflow.keras — avoid mixing keras.*)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    EinsumDense,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    Add,\n",
    "    GlobalAveragePooling1D,\n",
    "    Masking,\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import (\n",
    "    TerminateOnNaN,\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler,\n",
    "    LambdaCallback,\n",
    ")\n",
    "\n",
    "# Constraints\n",
    "from tensorflow.keras.constraints import MaxNorm, UnitNorm, MinMaxNorm\n",
    "\n",
    "# Utilities\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Scikit-learn\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.preprocessing import LabelBinarizer, label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# HGQ + +DA4ML + HLS4ML\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "from hgq.config import QuantizerConfig, QuantizerConfigScope\n",
    "from hgq.layers import QDense, QSoftmax, QEinsumDenseBatchnorm\n",
    "from hgq.utils.sugar import BetaScheduler, Dataset, FreeEBOPs, ParetoFront, PBar, PieceWiseSchedule\n",
    "\n",
    "from hgq.regularizers import MonoL1\n",
    "#from da4ml.codegen import VerilogModel\n",
    "#from da4ml.converter.hgq2.parser import trace_model\n",
    "#from da4ml.trace import HWConfig, comb_trace\n",
    "from hls4ml.converters import convert_from_keras_model\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Environment setup (before TensorFlow import)\n",
    "# ====================================================\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"   # 0=all, 1=info, 2=warning, 3=error\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force CPU only (disable Metal GPU)\n",
    "os.environ[\"TF_METAL_ENABLE\"] = \"0\"        # disables Metal plugin (macOS)\n",
    "#os.environ['KERAS_BACKEND'] = 'jax'         # Sets Jax as Keras backend \n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_auto_jit=-1\"\n",
    "# ====================================================\n",
    "# TensorFlow version\n",
    "# ====================================================\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "# ====================================================\n",
    "# JAX devices and \n",
    "# ====================================================\n",
    "#print(\"jax devices:\", jax.devices())           # should show METAL device(s) if jax-metal active\n",
    "\n",
    "# ====================================================\n",
    "# TensorFlow Debugging \n",
    "# ====================================================\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# ====================================================\n",
    "# Background monitor for CPU and RAM\n",
    "# ====================================================\n",
    "#def monitor_resources(interval=2):\n",
    "#    \"\"\"Print CPU and memory usage every `interval` seconds.\"\"\"\n",
    "#    while True:\n",
    "#        mem = psutil.virtual_memory()\n",
    "#        cpu = psutil.cpu_percent()\n",
    "#        print(f\"[Resource Monitor] CPU: {cpu:.1f}% | RAM Used: {mem.used/1e9:.2f} GB / {mem.total/1e9:.2f} GB\")\n",
    "#        time.sleep(interval)\n",
    "#\n",
    "# Start monitoring in background thread\n",
    "#thread = threading.Thread(target=monitor_resources, daemon=True)\n",
    "#thread.start()\n",
    "\n",
    "# JAX Keras backend\n",
    "#if keras.backend.backend() == 'jax':\n",
    "#    jax.config.update(\"jax_default_matmul_precision\", \"tensorfloat32\")\n",
    "\n",
    "print(\"keras backend:\", keras.config.backend())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664197f9-1bfe-4b1f-83ce-4a463e0a447a",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5309b436-cd2b-4f53-a8d8-604623033caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config\n",
    "seed = 123\n",
    "seed = 123\n",
    "first_decay_steps = 100\n",
    "merit = 'loss'\n",
    "# merit = 'accuracy'\n",
    "# merit = \n",
    "#cls_output_categorical_accuracy\n",
    "#cls_output_loss\n",
    "#loss\n",
    "#pt_output_loss\n",
    "#pt_output_mean_squared_error\n",
    "#val_cls_output_categorical_accuracy\n",
    "#val_cls_output_loss\n",
    "#val_loss,val_pt_output_loss\n",
    "#val_pt_output_mean_squared_error\n",
    "\n",
    "bsz =128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ebf81-b70c-43c3-b9e8-ca6070f2216d",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abdd7610-bffc-4486-9b18-7f6f138d2e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading class: Top\n",
      "Top: constituents (2815, 16, 5), jets (2815, 11), labels (2815,)\n",
      "\n",
      "Loading class: HQQ\n",
      "HQQ: constituents (100000, 16, 5), jets (100000, 11), labels (100000,)\n",
      "\n",
      "Loading class: HTauTau\n",
      "HTauTau: constituents (6659, 16, 5), jets (6659, 11), labels (6659,)\n",
      "\n",
      "Loading class: Wqq\n",
      "Wqq: constituents (18838, 16, 5), jets (18838, 11), labels (18838,)\n",
      "\n",
      "Loading class: QCD\n",
      "QCD: constituents (100000, 16, 5), jets (100000, 11), labels (100000,)\n",
      "\n",
      "Shapes before train/test split:\n",
      "X_constituents: (228312, 16, 5)\n",
      "X_jets: (228312, 11)\n",
      "y: (228312, 5)\n",
      "\n",
      "Shapes after train/test split:\n",
      "X_train_val: (159818, 16, 5) Xc_test: (68494, 16, 5)\n",
      "Xj_train: (159818, 11) Xj_test: (68494, 11)\n",
      "y_train_val (classification): (159818, 5) y_test: (68494, 5)\n",
      "y_reg_train_val (regression): (159818,) y_reg_test: (68494,)\n",
      " \n",
      "----------------------------------------------------------------------------------------\n",
      " \n",
      "y_train_val[:10] (regression): [[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]] y_test[:10]: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "y_reg_train_val[:10] (regression): [397.    61.75 662.75 372.5  273.   175.   464.25 170.25 361.   379.  ] y_reg_test[:10]: [ 52.    31.25 143.    54.    41.5  427.5   66.75 575.25 412.75  73.25]\n"
     ]
    }
   ],
   "source": [
    "#Data PATH\n",
    "inputdir = '/Users/sznajder/cernbox/Temp/scaled_classes_perjet'\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1. Classes and mapping to integer indices\n",
    "# ------------------------------------------\n",
    "class_labels = {\n",
    "    \"Top\": 10,\n",
    "    \"HQQ\": 20,\n",
    "    \"HTauTau\": 40,\n",
    "    \"Wqq\": 50,\n",
    "    \"QCD\": 70,\n",
    "}\n",
    "\n",
    "label_to_idx = {label: i for i, label in enumerate(class_labels.values())}\n",
    "nclasses = len(class_labels)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2. Features\n",
    "# ------------------------------------------\n",
    "#constituents_features_list = [\n",
    "#    'pt','pt_phys','pt_rel','eta_phys','deta_phys','phi_phys','dphi_phys',\n",
    "#    'dr','z0','dxy','dxy_custom','id','charge','track_vx','track_vy',\n",
    "#    'track_vz','track_d0','track_z0','puppiweight'\n",
    "#]\n",
    "\n",
    "constituents_features_list = ['pt_phys','pt_rel','deta_phys','dphi_phys','dxy']\n",
    "\n",
    "jet_features_list = [\n",
    "    \"jet_pt\", \"jet_eta\", \"jet_phi\", \"jet_mass\", \"jet_energy\", \"jet_bjetscore\",\n",
    "    \"jet_genmatch_pt\", \"jet_genmatch_eta\", \"jet_genmatch_phi\",\n",
    "    \"jet_genmatch_mass\", \"jet_genmatch_flav\"\n",
    "]\n",
    "\n",
    "outdir = \"./scaled_classes_perjet\"\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3. Load per-class parquet files and build arrays\n",
    "# ------------------------------------------\n",
    "constituents_list = []\n",
    "jets_list = []\n",
    "labels_list = []\n",
    "nmax = 100000 # max number of jets per class\n",
    "weights_list = []  # weights to balance classes during training\n",
    "\n",
    "for cls_name, cls_label in class_labels.items():\n",
    "    print(f\"\\nLoading class: {cls_name}\")\n",
    "\n",
    "    # Load constituents\n",
    "    const_file = os.path.join(inputdir, f\"{cls_name}_Constituents.parquet\")\n",
    "    constituents = ak.from_parquet(const_file)\n",
    "    constituents_array = np.stack([np.ascontiguousarray(constituents[f]) for f in constituents_features_list], axis=-1)\n",
    "\n",
    "    # Load jet-level features\n",
    "    jets_file = os.path.join(inputdir, f\"{cls_name}_Jets.parquet\")\n",
    "    jets = ak.from_parquet(jets_file)\n",
    "    jets_array = np.stack([np.ascontiguousarray(jets[f]) for f in jet_features_list], axis=-1)\n",
    "\n",
    "    # Labels (consecutive indices)\n",
    "    label_idx = label_to_idx[cls_label]\n",
    "    labels_array = np.full(jets_array.shape[0], label_idx)\n",
    "\n",
    "    # Filter out up to nmax jets per class\n",
    "    constituents_array = constituents_array[0:nmax]\n",
    "    jets_array = jets_array[0:nmax]\n",
    "    labels_array = labels_array[0:nmax]\n",
    "    \n",
    "    # Append to lists\n",
    "    constituents_list.append(constituents_array)\n",
    "    jets_list.append(jets_array)\n",
    "    labels_list.append(labels_array)\n",
    "\n",
    "    # Define class weight\n",
    "    weight = 1./len(labels_array)\n",
    "    weights_list.append( np.ones(len(labels_array))*weight )\n",
    "\n",
    "    \n",
    "    print(f\"{cls_name}: constituents {constituents_array.shape}, jets {jets_array.shape}, labels {labels_array.shape}\")\n",
    "\n",
    "\n",
    "# Th=ransform the list of weights into sample weights\n",
    "class_weights = dict(enumerate(weights_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4a. Concatenate all classes \n",
    "# ------------------------------------------\n",
    "X_constituents = np.concatenate(constituents_list, axis=0).astype(np.float32)\n",
    "X_jets = np.concatenate(jets_list, axis=0).astype(np.float32)\n",
    "labels_array = np.concatenate(labels_list, axis=0)\n",
    "weights = np.concatenate(weights_list, axis=0)\n",
    "\n",
    "\n",
    "# One-hot encode labels for Keras\n",
    "y = to_categorical(labels_array, num_classes=nclasses).astype(np.float32)\n",
    "\n",
    "\n",
    "# Data shapes\n",
    "nconstit = X_constituents.shape[-2]\n",
    "nfeat = X_constituents.shape[-1]\n",
    "\n",
    "print(\"\\nShapes before train/test split:\")\n",
    "print(\"X_constituents:\", X_constituents.shape)\n",
    "print(\"X_jets:\", X_jets.shape)\n",
    "print(\"y:\", y.shape)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4b. Create regression target: jet pT\n",
    "# ------------------------------------------\n",
    "# Assuming X_jets has jet-level features in the order:\n",
    "# [\"jet_pt\", \"jet_eta\", \"jet_phi\", ...] as defined in jet_features_list\n",
    "# We take the first column (jet_pt) as regression target\n",
    "y_reg = X_jets[:, 0].astype(np.float32)  # shape (n_jets,)\n",
    "#y_reg = y_reg.reshape(-1, 1)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 5. Split into train/test (stratified)\n",
    "# ------------------------------------------\n",
    "X_train_val, X_test, \\\n",
    "Xj_train_val, Xj_test, y_train_val, \\\n",
    "y_test, y_reg_train_val, y_reg_test, \\\n",
    "weights_train_val, weights_test = train_test_split(\n",
    "    X_constituents,\n",
    "    X_jets,\n",
    "    y,\n",
    "    y_reg,\n",
    "    weights,\n",
    "    test_size=0.3,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "#    stratify=labels_array\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_val, \\\n",
    "Xj_train, Xj_val, \\\n",
    "y_train, y_val, \\\n",
    "y_reg_train, y_reg_val, \\\n",
    "weights_train, weights_val = train_test_split(\n",
    "    X_train_val,\n",
    "    Xj_train_val,\n",
    "    y_train_val,\n",
    "    y_reg_train_val,\n",
    "    weights_train_val,\n",
    "    test_size=0.3,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "#    stratify=y_train_val \n",
    ")\n",
    "\n",
    "# Sets constant weights for regression \n",
    "weights_reg_train = np.ones( len(y_train) )\n",
    "weights_reg_val = np.ones( len(y_val) )\n",
    "\n",
    "# Do this once before the loop:\n",
    "#X_train, X_val, y_cls_train, y_cls_val, y_reg_train, y_reg_val = train_test_split(\n",
    "#    X_train_val, y_train_val, y_reg_train_val, test_size=0.3, stratify=np.argmax(y_train_val, axis=1), random_state=42)\n",
    "\n",
    "nsamples = len(X_train_val)\n",
    "\n",
    "'''\n",
    "def make_dataset(X, y_cls, y_reg, batch_size=bsz, shuffle=True):\n",
    "    # y_cls: (N, nclasses) one-hot\n",
    "    # y_reg: (N,1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, {\"cls_output\": y_cls, \"pt_output\": y_reg}))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_dataset = make_dataset(X_train, y_train, y_reg_train, batch_size=bsz, shuffle=True)\n",
    "val_dataset = make_dataset(X_val, y_val, y_reg_val, batch_size=bsz, shuffle=True)\n",
    "test_dataset  = make_dataset(X_test     , y_test     , y_reg_test     , batch_size=bsz, shuffle=False)\n",
    "'''\n",
    "\n",
    "print(\"\\nShapes after train/test split:\")\n",
    "print(\"X_train_val:\", X_train_val.shape, \"Xc_test:\", X_test.shape)\n",
    "print(\"Xj_train:\", Xj_train_val.shape, \"Xj_test:\", Xj_test.shape)\n",
    "print(\"y_train_val (classification):\", y_train_val.shape, \"y_test:\", y_test.shape)\n",
    "print(\"y_reg_train_val (regression):\", y_reg_train_val.shape, \"y_reg_test:\", y_reg_test.shape)\n",
    "print(\" \")\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "print(\" \")\n",
    "print(\"y_train_val[:10] (regression):\", y_train_val[:10], \"y_test[:10]:\", y_test[:10])\n",
    "print(\"y_reg_train_val[:10] (regression):\", y_reg_train_val[:10], \"y_reg_test[:10]:\", y_reg_test[:10])\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6. Arrays ready for Keras\n",
    "# X_train_val / X_test → (n_jets, Nconstit, n_features)\n",
    "# Xj_train_val / Xj_test → (n_jets, n_jet_features)\n",
    "# y_train_val / y_test   → (n_jets, n_classes)\n",
    "# ------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7985931-a75f-4952-acfc-d2a57502e9c0",
   "metadata": {},
   "source": [
    "## Define the Models\n",
    "#### https://github.com/calad0i/JEDI-linear/blob/master/src/model.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0796e9e7-4f93-48b3-8473-634a98fe89f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Skipping these should also work.\n",
    "# Usually, the default configs are good enough for most cases, but the initial number of bits, `[bif]0`\n",
    "# may need to be increased. If you see that the model is not converging, you can try increasing these values.\n",
    "#\n",
    "#scope0 = QuantizerConfigScope(place='all', k0=1, b0=3, i0=0, default_q_type='kbi', overflow_mode='sat_sym')\n",
    "#scope1 = QuantizerConfigScope(place='datalane', k0=0, default_q_type='kif', overflow_mode='wrap', f0=3, i0=3)\n",
    "#\n",
    "#iq_conf = QuantizerConfig(place='datalane', round_mode='RND')\n",
    "#iq_default = QuantizerConfig(place='datalane')\n",
    "#\n",
    "#\n",
    "#exp_table_conf = QuantizerConfig('kif', 'table', k0=0, i0=1, f0=8, overflow_mode='sat_sym')\n",
    "#inv_table_conf = QuantizerConfig('kif', 'table', k0=1, i0=4, f0=4, overflow_mode='sat_sym')\n",
    "#\n",
    "#iq_conf = QuantizerConfig(place='datalane', k0=1)\n",
    "#oq_conf = QuantizerConfig(place='datalane', k0=1, fr=MonoL1(1e-3))\n",
    "#\n",
    "# Layer scope will over formal one. When using scope0, scope1, 'datalane' config will be overriden with config in scope1\n",
    "#lscope = LayerConfigScope(enable_ebops=True, beta0=1e-5)\n",
    "#\n",
    "\n",
    "\n",
    "# From https://github.com/calad0i/HGQ2-examples/blob/master/jsc/model.py\n",
    "#init_bw_k=3\n",
    "#init_bw_a=3\n",
    "init_bw_k=8\n",
    "init_bw_a=8\n",
    "#beta0=1e-5\n",
    "# \n",
    "scope0 = QuantizerConfigScope(place='weight', \n",
    "                              overflow_mode='SAT_SYM', \n",
    "                              f0=init_bw_k, \n",
    "                              trainable=True)\n",
    "scope1 = QuantizerConfigScope(place='bias', \n",
    "                              overflow_mode='WRAP', \n",
    "                              f0=init_bw_k, \n",
    "                              trainable=True)\n",
    "scope2 = QuantizerConfigScope(place='datalane', \n",
    "                              i0=8, \n",
    "                              f0=init_bw_a)\n",
    "#\n",
    "#scope0 = QuantizerConfigScope(default_q_type='dummy')\n",
    "#scope1 = QuantizerConfigScope(default_q_type='dummy')\n",
    "#scope2 = QuantizerConfigScope(default_q_type='dummy')\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################################################################\n",
    "\n",
    "#### # MLP using Dense layers\n",
    "def MLP_dense(nhid=64):\n",
    "    with (scope0, scope1, scope2):\n",
    "\n",
    "        inp = keras.layers.Input((nconstit, nfeat))\n",
    "\n",
    "#    masked_inp = Masking(mask_value=-999, name=\"masking\")(inp) \n",
    "        x = BatchNormalization()(inp)\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        xr = QDense( nhid, activation='relu', name=\"pt_dense1\" )(x)\n",
    "        xr = QDense( nhid, activation='relu', name=\"pt_dense2\" )(xr)\n",
    "        pt_out = QDense(1, name=\"pt_output\")(xr)     # Pt regression head\n",
    "\n",
    "        x = QDense( nhid, activation='relu', name=\"cls_dense1\")(x)\n",
    "        x = QDense( nhid, activation='relu', name=\"cls_dense2\" )(x)\n",
    "        x = QDense( nhid, activation='relu', name=\"cls_dense3\" )(x)\n",
    "        cls_out = QDense(nclasses, name=\"cls_output\")(x) # classification head\n",
    " \n",
    "    model = keras.Model(inputs=inp, outputs=[cls_out, pt_out])\n",
    "    return model\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "# MLP using EinsumDense layers \n",
    "def MLP_einsum(nhid=64):\n",
    "\n",
    "    with (scope0, scope1, scope2):\n",
    "\n",
    "        inp = keras.layers.Input((nconstit, nfeat))\n",
    "\n",
    "#        x = BatchNormalization()(inp)\n",
    "        x = inp\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        # Pt Regression\n",
    "        xr = QEinsumDenseBatchnorm('bc,cF->bF', nhid, bias_axes='F', activation='relu', name=\"pt_dense1\" )(x)\n",
    "        xr = QEinsumDenseBatchnorm('bc,cF->bF', nhid, bias_axes='F', activation='relu', name=\"pt_dense2\" )(xr)\n",
    "        pt_out = QEinsumDenseBatchnorm('bc,cF->bF', 1, bias_axes='F', name=\"pt_output\" )(xr)      # Pt regression head\n",
    "\n",
    "        # Classifier\n",
    "        x = QEinsumDenseBatchnorm('bc,cF->bF', nhid, bias_axes='F', activation='relu', name=\"cls_einsum1\" )(x)\n",
    "        x = QEinsumDenseBatchnorm('bc,cF->bF', nhid, bias_axes='F', activation='relu', name=\"cls_einsum2\" )(x)\n",
    "        x = QEinsumDenseBatchnorm('bc,cF->bF', nhid, bias_axes='F', activation='relu', name=\"cls_einsum3\" )(x)\n",
    "        cls_out = QEinsumDenseBatchnorm('bc,cF->bF', nclasses, bias_axes='F', name=\"cls_output\" )(x)  # classification head\n",
    "\n",
    "    model = keras.Model(inputs=inp, outputs=[cls_out, pt_out])\n",
    "    return model\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "def DS_dense(nhid=64):\n",
    "\n",
    "    with (scope0, scope1, scope2):\n",
    "\n",
    "        inp = keras.layers.Input((nconstit, nfeat))\n",
    "\n",
    "#        x = BatchNormalization()(inp)\n",
    "        x = inp\n",
    "\n",
    "        x = QDense(nhid, activation='relu', name=\"dense1\")(x)\n",
    "        x = QDense(nhid, activation='relu', name=\"dense2\")(x)\n",
    "        x = QDense(nhid, activation='relu', name=\"dense3\")(x)\n",
    "#        x = tf.keras.sum(x, axis=1, keepdims=False) / N\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "        # Pt Regression\n",
    "        xr = QDense(nhid, activation='relu', name=\"pt_dense1\" )(x)\n",
    "        xr = QDense(nhid, activation='relu', name=\"pt_dense2\" )(xr)\n",
    "        pt_out = QDense(1, name=\"pt_output\")(xr)     # Pt regression head\n",
    "\n",
    "    # Classifier\n",
    "    x = QDense(nhid, activation='relu', name=\"cls_dense1\")(x)\n",
    "    x = QDense(nhid, activation='relu', name=\"cls_dense2\")(x)\n",
    "    cls_out = QDense(nclasses, name=\"cls_output\")(x) # classification head\n",
    "\n",
    "    model = keras.Model(inputs=inp, outputs=[cls_out, pt_out])\n",
    "    return model\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "def DS_einsum(nhid=64):\n",
    "\n",
    "    with (scope0, scope1, scope2):\n",
    "\n",
    "        N=nconstit\n",
    "        inp = keras.layers.Input((nconstit, nfeat))\n",
    "    \n",
    "#        x = BatchNormalization()(inp)\n",
    "        x = inp\n",
    "\n",
    "        x = QEinsumDenseBatchnorm('bnc,cC->bnC', (N, nhid), bias_axes='C', activation='relu', name=\"einsum1\")(x)\n",
    "        x = QEinsumDenseBatchnorm('bnc,cC->bnC', (N, nhid), bias_axes='C', activation='relu', name=\"einsum2\")(x)\n",
    "        x = QEinsumDenseBatchnorm('bnc,cC->bnC', (N, nhid), bias_axes='C', activation='relu', name=\"einsum3\")(x)\n",
    "#        x = tf.keras.sum(x, axis=1, keepdims=False) / N\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "        # Pt Regression\n",
    "        xr = QEinsumDenseBatchnorm('bc,cC->bC', nhid, bias_axes='C', activation='relu', name=\"pt_dense1\" )(x)\n",
    "        xr = QEinsumDenseBatchnorm('bc,cC->bC', nhid, bias_axes='C', activation='relu', name=\"pt_dense2\" )(xr)\n",
    "        pt_out = QEinsumDenseBatchnorm('bc,cC->bC', 1, bias_axes='C', name=\"pt_output\" )(xr)      # Pt regression head\n",
    "\n",
    "        # Classifier\n",
    "        x = QEinsumDenseBatchnorm('bc,cC->bC', nhid, bias_axes='C', activation='relu', name=\"cls_einsum1\")(x)\n",
    "        x = QEinsumDenseBatchnorm('bc,cC->bC', nhid, bias_axes='C', activation='relu', name=\"cls_einsum2\")(x)\n",
    "        cls_out = QEinsumDenseBatchnorm('bc,cC->bC', nclasses, bias_axes='C', name=\"cls_output\" )(x)  # classification head\n",
    "\n",
    "    model = keras.Model(inputs=inp, outputs=[cls_out, pt_out])\n",
    "    return model\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "# MLP mixer network\n",
    "def MLP_mixer(nhid=16):\n",
    "\n",
    "    with (scope0, scope1, scope2):\n",
    "\n",
    "        N=nconstit\n",
    "        inp = keras.layers.Input((nconstit, nfeat))\n",
    "\n",
    "#        x = BatchNormalization()(inp)\n",
    "        x = inp\n",
    "        \n",
    "        # Patches MLP\n",
    "        x1 = QEinsumDenseBatchnorm('bnc,cC->bnC', (N, nhid), bias_axes='C', activation='relu', name=\"einsum1\")(x)\n",
    "        x1 = QEinsumDenseBatchnorm('bnc,cC->bnC', (N, n), bias_axes='C', activation='relu', name=\"einsum2\")(x1)\n",
    "        x1 = QEinsumDenseBatchnorm('bnc,nN->bNc', (N, n), bias_axes='N', name=\"einsum3\")(x1)\n",
    "\n",
    "        # modification on original Cheng Sun MLP_MIXER model to add batch normed input \n",
    "        x = Add()([x, x1])\n",
    "\n",
    "        # Features MLP\n",
    "        x = QEinsumDenseBatchnorm('bnc,cC->bnC', (N, nhid), bias_axes='C', activation='relu', name=\"einsum4\" )(x)\n",
    "        x = QEinsumDenseBatchnorm('bnc,cC->bnC', (N, nhid), bias_axes='C', activation='relu', name=\"einsum5\")(x)\n",
    "        x = QEinsumDenseBatchnorm('bnc,n->bc', nhid, name=\"einsum6\")(x)\n",
    "\n",
    "        # Pt Regression\n",
    "        xr = QEinsumDenseBatchnorm('bc,cC->bC', nhid, bias_axes='C', activation='relu', name=\"pt_dense1\"  )(x)\n",
    "        xr = QEinsumDenseBatchnorm('bc,cC->bC', nhid, bias_axes='C', activation='relu', name=\"pt_dense2\"  )(xr)\n",
    "        pt_out = QEinsumDenseBatchnorm('bc,cC->bC', 1, bias_axes='C', name=\"pt_output\" )(xr)      # Pt regression head\n",
    "\n",
    "        # Classifier\n",
    "        x = QEinsumDenseBatchnorm('bc,cC->bC', nhid, bias_axes='C', activation='relu', name=\"cls_einsum1\")(x)\n",
    "        x = QEinsumDenseBatchnorm('bc,cC->bC', nhid, bias_axes='C', activation='relu', name=\"cls_einsum2\")(x)\n",
    "        x = QEinsumDenseBatchnorm('bc,cC->bC', nhid, bias_axes='C', activation='relu', name=\"cls_einsum3\")(x)\n",
    "        cls_out = QEinsumDenseBatchnorm('bc,cC->bC', nclasses, bias_axes='C', name=\"cls_output\" )(x)  # classification head\n",
    "\n",
    "    model = keras.Model(inputs=inp, outputs=[cls_out, pt_out])\n",
    "    return model\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "# JEDI linear InteractionNetwork\n",
    "def JEDI_linear(nhid=64):\n",
    "\n",
    "    with (scope0, scope1, scope2):\n",
    "\n",
    "        N=nconstit\n",
    "\n",
    "        pool_scale = 2.**-round(log2(N))\n",
    "    \n",
    "        inp = keras.layers.Input((nconstit, nfeat))\n",
    "\n",
    "#        x = BatchNormalization()(inp)\n",
    "        x = inp\n",
    "        \n",
    "        # Edges MLP\n",
    "        x = QEinsumDenseBatchnorm('bnc,cC->bnC', (N, nhid), bias_axes='C', activation='relu', name=\"einsum1\")(x)\n",
    "        s = QEinsumDenseBatchnorm('bnc,cC->bnC', (N, nhid), bias_axes='C', activation='relu', name=\"einsum2\")(x)\n",
    "        xx = tf.keras.ops.sum(x, axis=1, keepdims=True) / pool_scale\n",
    "        d = QEinsumDenseBatchnorm('bnc,cC->bnC', (1, nhid), bias_axes='C', activation='relu', name=\"einsum3\")(xx)\n",
    "\n",
    "        # Nodes MLP\n",
    "        x = Add()([s, d])\n",
    "        x = QEinsumDenseBatchnorm('bnc,cC->bnC', (N, nhid), bias_axes='C', activation='relu', name=\"einsum4\" )(x)\n",
    "        x = tf.keras.ops.sum(x, axis=1, keepdims=False) / N\n",
    "#        x = GlobalAveragePooling1D()(x)  # Why not use GlobalAverage Pooling1D instead o Sum above ?\n",
    "    \n",
    "\n",
    "        # Pt Regression\n",
    "        xr = QEinsumDenseBatchnorm('bc,cC->bC', int(nhid), bias_axes='C', activation='relu', name=\"pt_dense1\" )(x)\n",
    "        xr = QEinsumDenseBatchnorm('bc,cC->bC', int(nhid), bias_axes='C', activation='relu', name=\"pt_dense2\" )(xr)\n",
    "        pt_out = QEinsumDenseBatchnorm('bc,cC->bC', 1, bias_axes='C', name=\"pt_output\" )(xr)      # Pt regression head\n",
    "\n",
    "        # Graph classifier\n",
    "        x = QEinsumDenseBatchnorm('bc,cC->bC', nhid, bias_axes='C', activation='relu', name=\"cls_einsum1\")(x)\n",
    "        x = QEinsumDenseBatchnorm('bc,cC->bC', int(nhid), bias_axes='C', activation='relu', name=\"cls_einsum2\" )(x)\n",
    "        x = QEinsumDenseBatchnorm('bc,cC->bC', int(nhid), bias_axes='C', activation='relu', name=\"cls_einsum3\" )(x)\n",
    "        cls_out = QEinsumDenseBatchnorm('bc,cC->bC', nclasses, bias_axes='C' ,name='cls_output')(x)  # classification head\n",
    "\n",
    "    model = keras.Model(inputs=inp, outputs=[cls_out, pt_out])\n",
    "    return model\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "# Lipschitz MLP based on weights constrain/regularization\n",
    "@register_keras_serializable()\n",
    "class LipschitzReg(layers.Wrapper):\n",
    "    \"\"\"Wrapper around a provided Keras Dense layer with Lipschitz constraint on weights.\"\"\"\n",
    "    \n",
    "    def __init__(self, dense_layer, kind=\"one-inf\", **kwargs):\n",
    "        \"\"\"Initialize with a pre-defined Dense layer and norm constraint kind.\n",
    "        \n",
    "        Args:\n",
    "            dense_layer: A keras.layers.Dense instance to wrap.\n",
    "            kind: Norm constraint type (\"one\", \"inf\", \"one-inf\", \"two-inf\"). Default: \"one-inf\".\n",
    "                  - \"one\": L1 norm (|W|_1) constraint per row.\n",
    "                  - \"inf\": Linf norm (|W|_inf) constraint per column.\n",
    "                  - \"one-inf\": L1 norm per row, Linf per column.\n",
    "                  - \"two-inf\": L2 norm per row, Linf per column.\n",
    "        \"\"\"\n",
    "        self.kind = kind.lower()\n",
    "        if self.kind not in [\"one\", \"inf\", \"one-inf\", \"two-inf\"]:\n",
    "            raise ValueError(f\"Unsupported kind '{kind}'. Use 'one', 'inf', 'one-inf', or 'two-inf'.\")\n",
    "        # Validate and store the provided Dense layer\n",
    "        if not isinstance(dense_layer, layers.Dense):\n",
    "            raise ValueError(\"dense_layer must be an instance of keras.layers.Dense.\")\n",
    "        super(LipschitzReg, self).__init__(dense_layer, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build the wrapped Dense layer and normalize its weights.\"\"\"\n",
    "        super(LipschitzReg, self).build(input_shape)\n",
    "        self._normalize_weights()\n",
    "\n",
    "    def _normalize_weights(self):\n",
    "        \"\"\"Normalize the wrapped Dense layer's weights based on the specified kind.\"\"\"\n",
    "        W = self.layer.kernel\n",
    "        if self.kind == \"one\":\n",
    "            row_norms = tf.reduce_sum(tf.abs(W), axis=1, keepdims=True)\n",
    "            self.layer.kernel.assign(W / tf.maximum(row_norms, tf.keras.backend.epsilon()))\n",
    "        elif self.kind == \"inf\":\n",
    "            col_norms = tf.reduce_max(tf.abs(W), axis=0, keepdims=True)\n",
    "            self.layer.kernel.assign(W / tf.maximum(col_norms, tf.keras.backend.epsilon()))\n",
    "        elif self.kind == \"one-inf\":\n",
    "            row_norms = tf.reduce_sum(tf.abs(W), axis=1, keepdims=True)\n",
    "            col_norms = tf.reduce_max(tf.abs(W), axis=0, keepdims=True)\n",
    "            W_normalized = W / tf.maximum(row_norms, tf.keras.backend.epsilon())\n",
    "            self.layer.kernel.assign(W_normalized / tf.maximum(col_norms, tf.keras.backend.epsilon()))\n",
    "        elif self.kind == \"two-inf\":\n",
    "            row_norms = tf.sqrt(tf.reduce_sum(tf.square(W), axis=1, keepdims=True))\n",
    "            col_norms = tf.reduce_max(tf.abs(W), axis=0, keepdims=True)\n",
    "            W_normalized = W / tf.maximum(row_norms, tf.keras.backend.epsilon())\n",
    "            self.layer.kernel.assign(W_normalized / tf.maximum(col_norms, tf.keras.backend.epsilon()))\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"Apply the wrapped Dense layer with dynamic weight normalization during training.\"\"\"\n",
    "        if training:\n",
    "            self._normalize_weights()  # Re-normalize weights each forward pass in training\n",
    "        return self.layer(inputs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.layer.compute_output_shape(input_shape)\n",
    "\n",
    "\n",
    "def MLP_LipschitzReg(nhid=64):\n",
    "\n",
    "    inp = keras.layers.Input((nconstit, nfeat))\n",
    "\n",
    "#    masked_inp = Masking(mask_value=-999, name=\"masking\")(inp) \n",
    "    x = BatchNormalization()(inp)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Pt Regression\n",
    "    xr = LipschitzReg(layers.Dense(int(nhid), activation='relu'), name=\"pt_dense1\", kind=\"inf\")(x)   \n",
    "    xr = LipschitzReg(layers.Dense(int(nhid), activation='relu'), name=\"pt_dense2\", kind=\"inf\")(xr)\n",
    "    pt_out = LipschitzReg(layers.Dense(1, activation='relu' ), name=\"pt_output\", kind=\"inf\")(xr)  # Pt regression head\n",
    "\n",
    "    \n",
    "    x = LipschitzReg(layers.Dense(nhid, activation='relu'), kind=\"one-inf\")(x)  # First layer with one-inf constraint\n",
    "    x = LipschitzReg(layers.Dense(int(nhid), activation='relu'), kind=\"inf\")(x)     # Subsequent layer with inf constraint\n",
    "    x = LipschitzReg(layers.Dense(int(nhid), activation='relu'), kind=\"inf\")(x)\n",
    "#    x = LipschitzMonotonic(layers.Dense(int(nhid), activation='relu'), kind=\"inf\")(x)\n",
    "    cls_out = LipschitzReg(layers.Dense(nclasses), name=\"cls_output\", kind=\"inf\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inp, outputs=[cls_out, pt_out])\n",
    "    return model\n",
    "\n",
    "\n",
    "#############################################################################################################\n",
    "#\t•\tUsing kernel_constraint=MaxNorm(1.0) is a cheap approximation for controlling the Lipschitz constant.\n",
    "#\t•\tIt does not guarantee 1-Lipschitz behavior, so the network is effectively K-Lipschitz, where K depends \n",
    "#         on the number of neurons and weight geometry.\n",
    "#\t•\tFor a provably 1-Lipschitz network, you need either:\n",
    "#\t•\tSpectral normalization (largest singular value ≤ 1)\n",
    "#\t•\tOr explicit row/column norm normalization like LipschitzMonotonic.\n",
    "\n",
    "# ---- GroupSort2 activation ----\n",
    "'''\n",
    "class GroupSort2(layers.Layer):\n",
    "    def call(self, x):\n",
    "        # x shape: (batch, features)\n",
    "        # Pad if number of features is odd\n",
    "        if x.shape[-1] % 2 != 0:\n",
    "            x = tf.pad(x, [[0,0],[0,1]], \"CONSTANT\")\n",
    "        features = tf.shape(x)[-1]\n",
    "        x = tf.reshape(x, (-1, features // 2, 2))  # group of 2\n",
    "        x = tf.sort(x, axis=-1)                        # sort each pair\n",
    "        return tf.reshape(x, (-1, x.shape[-1]*2))     # flatten\n",
    "'''\n",
    "@register_keras_serializable()\n",
    "class GroupSort2(layers.Layer):\n",
    "    \"\"\"GroupSort activation with group size 2.\"\"\"\n",
    "\n",
    "    def call(self, x):\n",
    "        # Number of channels (last dimension)\n",
    "        channels = tf.shape(x)[-1]\n",
    "\n",
    "        # Pad if odd number of channels\n",
    "        needs_pad = channels % 2\n",
    "\n",
    "        # Construct padding dynamically for arbitrary tensor rank\n",
    "        pad = tf.concat([\n",
    "            tf.zeros([tf.rank(x) - 1, 2], dtype=tf.int32),  # no pad for all axes except last\n",
    "            [[needs_pad, 0]]                               # pad last dimension\n",
    "        ], axis=0)\n",
    "\n",
    "        # Always pad, easier than tf.cond\n",
    "        x_pad = tf.pad(x, pad)\n",
    "\n",
    "        # Split into pairs\n",
    "        a, b = tf.split(x_pad, 2, axis=-1)\n",
    "\n",
    "        # Sort each pair\n",
    "        y = tf.concat([tf.minimum(a, b), tf.maximum(a, b)], axis=-1)\n",
    "\n",
    "        # Remove padding\n",
    "        return y[..., :channels]\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# Spectral Normalization can be approximated using a custom constraint\n",
    "def SpectralDense(units, name=None):\n",
    "    return layers.Dense(units, activation=None, kernel_constraint=MaxNorm(max_value=1.0), name=name)\n",
    "    \n",
    "def MLP_LipschitzGroupSort(nhid=64):\n",
    "\n",
    "    inp = keras.layers.Input(shape=(nconstit, nfeat))\n",
    "\n",
    "    # --- Flatten + normalization ---\n",
    "    x = layers.BatchNormalization()(inp)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # ========== Regression tower ==========\n",
    "    xr = SpectralDense(nhid, name=\"pt_dense1\")(x)\n",
    "    xr = GroupSort2()(xr)\n",
    "    xr = SpectralDense(nhid, name=\"pt_dense2\")(xr)\n",
    "    xr = GroupSort2()(xr)\n",
    "    pt_out = SpectralDense(1, name=\"pt_output\")(xr)   # Linear output, no activation\n",
    "\n",
    "\n",
    "    # ========== Classification tower ==========\n",
    "    xc = SpectralDense(nhid, name=\"cls_dense1\")(x)\n",
    "    xc = GroupSort2()(xc)\n",
    "    xc = SpectralDense(nhid, name=\"cls_dense2\")(xc)\n",
    "    xc = GroupSort2()(xc)\n",
    "    cls_out = SpectralDense(nclasses, name=\"cls_output\")(xc)   # logits\n",
    "\n",
    "    # --- Combined model ---\n",
    "    model = keras.Model(inputs=inp, outputs=[cls_out, pt_out])\n",
    "    return model   \n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "def cosine_decay_restarts_schedule(\n",
    "    initial_learning_rate: float, first_decay_steps: int, t_mul=1.0, m_mul=1.0, alpha=0.0, alpha_steps=0\n",
    "):\n",
    "    def schedule(global_step):\n",
    "        n_cycle = 1\n",
    "        cycle_step = global_step\n",
    "        cycle_len = first_decay_steps\n",
    "        while cycle_step >= cycle_len:\n",
    "            cycle_step -= cycle_len\n",
    "            cycle_len *= t_mul\n",
    "            n_cycle += 1\n",
    "\n",
    "        cycle_t = min(cycle_step / (cycle_len - alpha_steps), 1)\n",
    "        lr = alpha + 0.5 * (initial_learning_rate - alpha) * (1 + cos(pi * cycle_t)) * m_mul ** max(n_cycle - 1, 0)\n",
    "        return lr\n",
    "\n",
    "    return schedule\n",
    "\n",
    "\n",
    "#############################################################################################################\n",
    "from tensorflow.keras.models import Model\n",
    "def inspect_layer_outputs(model, sample_inputs, layers_to_check=None, show_sample=True):\n",
    "    \"\"\"\n",
    "    Prints layer-wise output statistics for a given Keras model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.Model\n",
    "        Your trained model.\n",
    "    sample_inputs : np.array\n",
    "        Input samples to feed through the model.\n",
    "    layers_to_check : list of str, optional\n",
    "        Names of layers to inspect. If None, all layers are inspected.\n",
    "    show_sample : bool\n",
    "        Whether to print the first sample's output in detail.\n",
    "    \"\"\"\n",
    "    # Choose layers to inspect\n",
    "    if layers_to_check is None:\n",
    "        layers_to_check = [layer.name for layer in model.layers]\n",
    "\n",
    "    # Get outputs of the chosen layers\n",
    "    layer_outputs = [model.get_layer(name).output for name in layers_to_check]\n",
    "\n",
    "    # Create a temporary model that returns all layer outputs\n",
    "    inspect_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = inspect_model.predict(sample_inputs, verbose=0)\n",
    "\n",
    "    # Print layer-wise statistics\n",
    "    for layer_name, out in zip(layers_to_check, outputs):\n",
    "        print(f\"\\nLayer: {layer_name}\")\n",
    "        print(f\" Output shape: {out.shape}\")\n",
    "        print(f\" Min: {out.min():.6f}, Max: {out.max():.6f}, Mean: {out.mean():.6f}\")\n",
    "        if show_sample:\n",
    "            print(f\" First sample output (flattened): {out[0].flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a22192-ae8b-48df-bb00-ffbf192a8666",
   "metadata": {},
   "source": [
    "## Train the Models\n",
    "#### https://github.com/calad0i/JEDI-linear/blob/master/src/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcbda445-95e4-4c51-8fb4-ebeb602cd1ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed...\n",
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cls_dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QDense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,556</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cls_dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QDense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,046</span> │ cls_dense1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pt_dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QDense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,556</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cls_dense3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QDense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,046</span> │ cls_dense2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pt_dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QDense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,046</span> │ pt_dense1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cls_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QDense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">898</span> │ cls_dense3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pt_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QDense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">274</span> │ pt_dense2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m5\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m5\u001b[0m)     │         \u001b[38;5;34m20\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cls_dense1 (\u001b[38;5;33mQDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)        │     \u001b[38;5;34m12,556\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cls_dense2 (\u001b[38;5;33mQDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)        │      \u001b[38;5;34m6,046\u001b[0m │ cls_dense1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pt_dense1 (\u001b[38;5;33mQDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)        │     \u001b[38;5;34m12,556\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cls_dense3 (\u001b[38;5;33mQDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)        │      \u001b[38;5;34m6,046\u001b[0m │ cls_dense2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pt_dense2 (\u001b[38;5;33mQDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)        │      \u001b[38;5;34m6,046\u001b[0m │ pt_dense1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cls_output (\u001b[38;5;33mQDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m898\u001b[0m │ cls_dense3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pt_output (\u001b[38;5;33mQDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m274\u001b[0m │ pt_dense2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,442</span> (140.83 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,442\u001b[0m (140.83 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,672</span> (127.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m32,672\u001b[0m (127.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,770</span> (13.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m11,770\u001b[0m (13.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-training Model accuracy: 18.50%\n",
      "Regression MAE: 235.22256469726562,  MSE: 84828.8046875,  R2: -1.873410940170288\n",
      "Pre-Training the model classification: MLP_dense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/10 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m864/874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - cls_output_accuracy: 0.5534 - cls_output_loss: 2.6811e-05 - loss: 0.0071 - pt_output_loss: 26.5830 - pt_output_mean_squared_logarithmic_error: 26.5830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.007/0.000 - acc: 0.705/0.882 - lr: 1.00e-03 - beta: 1.0e-08 - EBOPs: 658/Users/sznajder/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/keras/src/callbacks/model_checkpoint.py:276: UserWarning: Can save best model only with val_accuracy available.\n",
      "  if self._should_save_model(epoch, batch, logs, filepath):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m874/874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - cls_output_accuracy: 0.7051 - cls_output_loss: 2.1036e-05 - loss: 0.0070 - pt_output_loss: 26.6564 - pt_output_mean_squared_logarithmic_error: 26.6564 - val_cls_output_accuracy: 0.8817 - val_cls_output_loss: 1.2756e-05 - val_loss: 2.5190e-04 - val_pt_output_loss: 26.8561 - val_pt_output_mean_squared_logarithmic_error: 26.8557 - ebops: 658648.0000 - learning_rate: 0.0010 - beta: 1.0000e-08\n",
      "Epoch 2/10\n",
      "\u001b[1m864/874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - cls_output_accuracy: 0.8385 - cls_output_loss: 1.5157e-05 - loss: 0.0064 - pt_output_loss: 26.7502 - pt_output_mean_squared_logarithmic_error: 26.7502"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.006/0.000 - acc: 0.828/0.907 - lr: 1.00e-03 - beta: 1.0e-08 - EBOPs: 566"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m874/874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - cls_output_accuracy: 0.8283 - cls_output_loss: 1.5678e-05 - loss: 0.0061 - pt_output_loss: 26.7865 - pt_output_mean_squared_logarithmic_error: 26.7865 - val_cls_output_accuracy: 0.9071 - val_cls_output_loss: 1.2280e-05 - val_loss: 2.2055e-04 - val_pt_output_loss: 26.9193 - val_pt_output_mean_squared_logarithmic_error: 26.9188 - ebops: 566275.0000 - learning_rate: 1.0000e-03 - beta: 1.0000e-08\n",
      "Epoch 3/10\n",
      "\u001b[1m864/874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - cls_output_accuracy: 0.8555 - cls_output_loss: 1.3844e-05 - loss: 0.0057 - pt_output_loss: 26.8047 - pt_output_mean_squared_logarithmic_error: 26.8047"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.006/0.000 - acc: 0.858/0.928 - lr: 1.00e-03 - beta: 1.0e-08 - EBOPs: 548"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m874/874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - cls_output_accuracy: 0.8575 - cls_output_loss: 1.3597e-05 - loss: 0.0057 - pt_output_loss: 26.8346 - pt_output_mean_squared_logarithmic_error: 26.8346 - val_cls_output_accuracy: 0.9277 - val_cls_output_loss: 1.0446e-05 - val_loss: 1.8799e-04 - val_pt_output_loss: 26.9454 - val_pt_output_mean_squared_logarithmic_error: 26.9449 - ebops: 548290.0000 - learning_rate: 1.0000e-03 - beta: 1.0000e-08\n",
      "Epoch 4/10\n",
      "\u001b[1m864/874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - cls_output_accuracy: 0.4090 - cls_output_loss: 2.5648e-05 - loss: 0.0332 - pt_output_loss: 26.8244 - pt_output_mean_squared_logarithmic_error: 26.8244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.031/0.000 - acc: 0.174/0.012 - lr: 1.00e-03 - beta: 7.0e-08 - EBOPs: 369"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m874/874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - cls_output_accuracy: 0.1744 - cls_output_loss: 3.1644e-05 - loss: 0.0312 - pt_output_loss: 26.8473 - pt_output_mean_squared_logarithmic_error: 26.8473 - val_cls_output_accuracy: 0.0120 - val_cls_output_loss: 3.5055e-05 - val_loss: 1.3021e-04 - val_pt_output_loss: 26.9391 - val_pt_output_mean_squared_logarithmic_error: 26.9386 - ebops: 369519.0000 - learning_rate: 1.0000e-03 - beta: 7.0000e-08\n",
      "Epoch 5/10\n",
      "\u001b[1m864/874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - cls_output_accuracy: 0.0122 - cls_output_loss: 3.5019e-05 - loss: 0.0294 - pt_output_loss: 26.8206 - pt_output_mean_squared_logarithmic_error: 26.8206"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.029/0.000 - acc: 0.012/0.012 - lr: 1.00e-03 - beta: 8.0e-08 - EBOPs: 298"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m874/874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - cls_output_accuracy: 0.0123 - cls_output_loss: 3.5198e-05 - loss: 0.0286 - pt_output_loss: 26.8456 - pt_output_mean_squared_logarithmic_error: 26.8456 - val_cls_output_accuracy: 0.0120 - val_cls_output_loss: 3.5055e-05 - val_loss: 7.1479e-05 - val_pt_output_loss: 26.9391 - val_pt_output_mean_squared_logarithmic_error: 26.9386 - ebops: 298673.0000 - learning_rate: 1.0000e-03 - beta: 8.0000e-08\n",
      "Epoch 6/10\n",
      "\u001b[1m864/874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - cls_output_accuracy: 0.0122 - cls_output_loss: 3.5019e-05 - loss: 0.0265 - pt_output_loss: 26.8206 - pt_output_mean_squared_logarithmic_error: 26.8206"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 159\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Fit the classification head\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mPre-Training the model classification:\u001b[39m\u001b[33m'\u001b[39m,name)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m history_class = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_reg_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_reg_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mweights_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweights_reg_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m                          \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mweights_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweights_reg_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# Call the function to inspact layer outputs after training to diagnose quantization problems\u001b[39;00m\n\u001b[32m    167\u001b[39m inspect_layer_outputs(model, X_test[:\u001b[32m5\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:423\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_eval_epoch_iterator\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    413\u001b[39m     \u001b[38;5;28mself\u001b[39m._eval_epoch_iterator = TFEpochIterator(\n\u001b[32m    414\u001b[39m         x=val_x,\n\u001b[32m    415\u001b[39m         y=val_y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    421\u001b[39m         shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    422\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m val_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m val_logs = {\n\u001b[32m    434\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs.items()\n\u001b[32m    435\u001b[39m }\n\u001b[32m    436\u001b[39m epoch_logs.update(val_logs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:511\u001b[39m, in \u001b[36mTensorFlowTrainer.evaluate\u001b[39m\u001b[34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    510\u001b[39m     callbacks.on_test_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m     callbacks.on_test_batch_end(end_step, logs)\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_evaluating:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1681\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1683\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1691\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1692\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1693\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1697\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1698\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/WorkM1/miniforge3/envs/tf2_18/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import keras\n",
    "import pickle as pkl\n",
    "import random\n",
    "\n",
    "print('Setting seed...')\n",
    "\n",
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "work_path = '/Users/sznajder/WorkM1/workdir/Notebooks/CMS_L1TML/Models_PerfNanoDataset'\n",
    "trained_models={}\n",
    "\n",
    "# Define models dictionary ( name and hyperparams ): Minimal models with about 10K parameters\n",
    "\n",
    "'''\n",
    "# Define models dictionary ( name and hyperparams ): Large Models\n",
    "models_parms = [ {'name': 'mlp_dense',  'nhid':128, 'lr': 0.001, 'nepochs': 100, 'bsz': 25},\n",
    "                 {'name': 'mlp_einsum', 'nhid':128, 'lr': 0.001, 'nepochs': 100, 'bsz': 258},\n",
    "                 {'name': 'ds_dense',   'nhid':96, 'lr': 0.001, 'nepochs': 100, 'bsz': 258} ,\n",
    "                 {'name': 'ds_einsum',  'nhid':96, 'lr': 0.001, 'nepochs': 100, 'bsz': 258} ,\n",
    "                 {'name': 'mlp_mixer',  'nhid':96, 'lr': 0.001, 'nepochs': 100, 'bsz': 258} ,\n",
    "                 {'name': 'gnn',        'nhid':96, 'lr': 0.001, 'nepochs': 100, 'bsz': 258} ,\n",
    "                 {'name': 'mlp_monotonic',   'nhid':64, 'lr': 0.001, 'nepochs': 100, 'bsz': 258} ]\n",
    "'''\n",
    "\n",
    "models_parms = [  {'name': 'MLP_dense',  'nhid':38, 'lr': 0.001, 'nepochs': 10},\n",
    "                 {'name': 'MLP_einsum', 'nhid':38, 'lr': 0.001, 'nepochs': 100},\n",
    "                 {'name': 'DS_dense',   'nhid':40, 'lr': 0.001, 'nepochs': 100} ,\n",
    "                 {'name': 'DS_einsum',  'nhid':40, 'lr': 0.001, 'nepochs': 100} ,\n",
    "                 {'name': 'MLP_mixer',  'nhid':38, 'lr': 0.001, 'nepochs': 100} ,\n",
    "                 {'name': 'JEDI_linear', 'nhid':38, 'lr': 0.001, 'nepochs': 100} ,\n",
    "#                 {'name': 'MLP_LipschitzReg',   'nhid':38, 'lr': 0.01, 'nepochs': 100} , \n",
    "#                 {'name': 'MLP_LipschitzGroupSort',   'nhid':38, 'lr': 0.01, 'nepochs': 100} \n",
    "                ]\n",
    "\n",
    "\n",
    "# Loop over models\n",
    "for m in models_parms:\n",
    "\n",
    "    # Get model name and hyperparameters\n",
    "    name, nhid, lr, nepochs = m['name'], m['nhid'], m['lr'], m['nepochs']\n",
    "\n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "    K.clear_session()\n",
    " \n",
    "    # Build the model and print its params\n",
    "    #    model = eval(name)(nhid)\n",
    "    model = globals()[name](nhid)\n",
    "    model.summary()\n",
    "\n",
    "    save_path = Path(work_path)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Define classification Loss \n",
    "    cls_loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "#    cls_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    '''\n",
    "    # Early stopping callback                       \n",
    "    es = EarlyStopping(monitor=merit, patience=20)\n",
    "\n",
    "    # Learning rate scheduler \n",
    "    ls = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=0.0000001, min_delta=0.02)\n",
    "    \n",
    "\n",
    "    # Define the Score output printou callback\n",
    "    print_outputs = LambdaCallback(on_epoch_end=lambda epoch, \n",
    "                                   logs: print(\"\\nEpoch {} - outputs:\\n{}\".format(epoch, model.predict(inputs))))\n",
    "    '''\n",
    "\n",
    "    \n",
    "    # Model checkpoint callback to save best model\n",
    "    chkp = ModelCheckpoint('./models/{}.keras'.format(name), \n",
    "                           monitor='val_accuracy', # best generalization \n",
    "                           mode='max', \n",
    "                           verbose=0, \n",
    "                           save_best_only=True, \n",
    "                           save_freq='epoch')\n",
    "                           \n",
    "    # Define list of callbacks\n",
    "#    callbk = [nan, es, ls, chkp]\n",
    "#    callbk = [es, ls, chkp]\n",
    "\n",
    "\n",
    "    \n",
    "    # HGQ callbacks\n",
    "    ebops = FreeEBOPs()\n",
    "\n",
    "    lr_sched = LearningRateScheduler(cosine_decay_restarts_schedule(lr, 4000, t_mul=1.0, m_mul=0.94, alpha=1e-6, alpha_steps=50))\n",
    "\n",
    "#    beta_sched = BetaScheduler(PieceWiseSchedule([(0, 5e-7, 'constant'), (4000, 5e-7, 'log'), (200000, 1e-3, 'constant')]))\n",
    "#    beta_sched = BetaScheduler(PieceWiseSchedule([(0, 2e-8, 'linear'), \n",
    "#                                                  (int(nepochs/2.), 3e-7, 'log'), \n",
    "#                                                  (nepochs, 3.0e-6, 'constant')]))\n",
    "    \n",
    "    beta_sched = BetaScheduler(PieceWiseSchedule([ (0, 1e-8, 'constant'),\n",
    "                                                   (int(nepochs/3.), 7e-8, 'linear'),\n",
    "                                                   (2*int(nepochs/3.), 1e-7, 'linear') ]))\n",
    "    \n",
    "#    pbar = PBar( 'loss: {loss:.2f}/{val_loss:.2f} - \\\n",
    "#                  acc: {accuracy:.4f}/{val_accuracy:.4f} - \\\n",
    "#                  lr: {learning_rate:.2e} - beta: {beta:.1e}')\n",
    "    pbar = PBar('loss: {loss:.3f}/{val_loss:.3f} - ' \\\n",
    "                'acc: {cls_output_accuracy:.3f}/{val_cls_output_accuracy:.3f} - ' \\\n",
    "                'lr: {learning_rate:.2e} - beta: {beta:.1e}')\n",
    "\n",
    "    # Define the ParetoFrontier for best models in terms of  'val_cls_output_accuracy' and 'ebops'\n",
    "#    pareto = ParetoFront( './train/',['val_accuracy', 'ebops'],[1, -1], \\\n",
    "#                          fname_format='epoch={epoch}-val_acc={val_accuracy:.3f}-ebops={ebops}-val_loss={val_loss:.3f}.keras')\n",
    "    fname = f'{name}-epoch={{epoch}}-val_acc={{val_cls_output_accuracy:.3f}}-ebops={{ebops}}-val_loss={{val_loss:.3f}}.keras'\n",
    "\n",
    "    pareto = ParetoFront( path='./models/', metrics=['val_cls_output_accuracy', 'ebops'], \n",
    "                          sides=[1, -1],  # maximize accuracy, minimize EBOPs \n",
    "                          fname_format=fname )\n",
    "\n",
    "    # Define model callbacks\n",
    "    callbk = [ebops, lr_sched, beta_sched, pbar, pareto, chkp ]\n",
    "\n",
    "   \n",
    "    # Intial classification and regression accuracies\n",
    "#    pred, pred_reg = model.predict(X_test, batch_size=bsz, verbose=0)  # type: ignore\n",
    "    pred, pred_reg = model.predict(X_test, batch_size=bsz, verbose=0)  # type: ignore\n",
    "    y_true = np.argmax(y_test, axis=1)  # true class indices\n",
    "    y_pred = np.argmax(pred, axis=1)  # predicted class indices\n",
    "    acc = np.mean(y_pred == y_true)\n",
    "    print(f'pre-training Model accuracy: {acc:.2%}')\n",
    "\n",
    "    mae = mean_absolute_error(y_reg_test, pred_reg)\n",
    "    mse = mean_squared_error(y_reg_test, pred_reg)\n",
    "    r2  = r2_score(y_reg_test, pred_reg)\n",
    "    print(f\"Regression MAE: {mae},  MSE: {mse},  R2: {r2}\")\n",
    "\n",
    "    \n",
    "    # Freeze regression layers\n",
    "#    for layer in model.layers:\n",
    "#            layer.trainable = True\n",
    "    for layer_name in [\"pt_output\", \"pt_dense1\", \"pt_dense2\"]:\n",
    "        model.get_layer(layer_name).trainable = False\n",
    "\n",
    "    # Compile Models for pre-training classification  \n",
    "#    opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr) # Faster on M1 Mac\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0) # Define optimizer\n",
    "    model.compile(optimizer=opt, \\\n",
    "                  loss={'cls_output': cls_loss,'pt_output':  'mean_squared_logarithmic_error'}, \\\n",
    "                  loss_weights={'cls_output': 1.0, 'pt_output': 0.0}, \\\n",
    "                  metrics={'cls_output': 'accuracy', 'pt_output':  'mean_squared_logarithmic_error'}, \\\n",
    "                  steps_per_execution=32)\n",
    "\n",
    "    # Fit the classification head\n",
    "    print('Pre-Training the model classification:',name)\n",
    "    history_class = model.fit(X_train, [y_train, y_reg_train], \n",
    "                              validation_data=(X_val, [y_val, y_reg_val], [weights_val,weights_reg_val]), \n",
    "                              sample_weight=[weights_train,weights_reg_train], \n",
    "                              batch_size=bsz, epochs=nepochs, callbacks=callbk, verbose=1)\n",
    "\n",
    "\n",
    "    \n",
    "    # Call the function to inspact layer outputs after training to diagnose quantization problems\n",
    "    inspect_layer_outputs(model, X_test[:5])\n",
    "\n",
    "    \n",
    "    # Unfreeze regression layers and freeze everithing else\n",
    "    for layer in model.layers:\n",
    "        if layer.name not in [\"pt_output\", \"pt_dense1\", \"pt_dense2\"]:\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "\n",
    "    # Compile Models for pre-training regression \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0) # Define optimizer\n",
    "    model.compile( optimizer=opt, \\\n",
    "                   loss={ \"cls_output\": cls_loss, 'pt_output': 'mean_squared_logarithmic_error'}, \\\n",
    "                   loss_weights={'cls_output': 0, 'pt_output': 1.0}, \\\n",
    "                   metrics={'cls_output': 'accuracy', 'pt_output': 'mean_squared_logarithmic_error' }, \\\n",
    "                   steps_per_execution=32 )\n",
    "\n",
    " \n",
    "    # Fit the regression head\n",
    "    print('Pre-Training the model regression:',name)\n",
    "    history_reg = model.fit(X_train, [y_train, y_reg_train], \\\n",
    "                            validation_data=(X_val, [y_val, y_reg_val], [weights_val,weights_reg_val]), \\\n",
    "                            sample_weight=[weights_train,weights_reg_train], \\\n",
    "                            batch_size=bsz, epochs=nepochs, callbacks=callbk, verbose=1)\n",
    "    '''\n",
    "    # Unfreeze all layers\n",
    "    for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "    \n",
    "    # Compile Models for fine-tunning complete model \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr/1000, clipnorm=1.0) # Define optimizer\n",
    "    model.compile( optimizer=opt, \\\n",
    "                   loss={ 'cls_output': cls_loss, 'pt_output': 'mean_squared_logarithmic_error'}, \\\n",
    "                   loss_weights={'cls_output': 0, 'pt_output': 1.0}, \\\n",
    "                   metrics={'cls_output': 'accuracy', 'pt_output': 'mean_squared_logarithmic_error' }, \\\n",
    "                   steps_per_execution=32 )\n",
    "\n",
    " \n",
    "    # Fit for fine-tune the complete model\n",
    "    print('Fine-Tune the model:',name)\n",
    "    history_finetune = model.fit(X_train, [y_train, y_reg_train], \\\n",
    "                                 validation_data=(X_val, [y_val, y_reg_val], [weights_val,weights_reg_val]), \\\n",
    "                                 sample_weight=[weights_train,weights_reg_train], \\\n",
    "                                 batch_size=bsz, epochs=nepochs, callbacks=callbk, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "#    with open(save_path / 'history.pkl', 'wb') as f:\n",
    "#        f.write(pkl.dumps(history))\n",
    "    '''\n",
    "    \n",
    "#    model.save('last_model.keras')\n",
    "\n",
    "    # After training finishes retrieve the best model saved by the checkpoint\n",
    "    best_model = load_model('./models/{}.keras'.format(name), custom_objects={\"LipschitzReg\": LipschitzReg})\n",
    "    trained_models[name] = best_model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3a116-9b4f-4504-a970-a234903f5242",
   "metadata": {},
   "source": [
    "## Test/Compare the Models\n",
    "#### https://github.com/calad0i/JEDI-linear/blob/master/src/test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd04f1-c8bf-4605-b52b-8ec1bb197bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Evaluate classification + regression for all models\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "results = {}\n",
    "\n",
    "fpr, tpr, roc_auc = {}, {}, {}\n",
    "inv_fpr_at_tpr = {}\n",
    "regression_metrics = {}\n",
    "\n",
    "n_classes = 5\n",
    "class_names = ['Top', 'HQQ', 'HTauTau', 'Wqq', 'QCD']\n",
    "target_tpr = 0.8\n",
    "\n",
    "# Binarize labels for ROC curves\n",
    "#y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))\n",
    "y_test_bin = y_test\n",
    "y_test_pt  = y_reg_test\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"Evaluating model {name} ...\")\n",
    "\n",
    "    # Predict\n",
    "    cls_pred_logits, pt_pred = model.predict(X_test, verbose=0)\n",
    "    cls_pred = tf.nn.softmax(cls_pred_logits, axis=-1).numpy()\n",
    "    pt_pred = pt_pred.reshape(-1)\n",
    "\n",
    "#    print(\"Y_TRUE=\",y_test_bin[:4])\n",
    "#    print(\"Y_PRED=\",cls_pred[:4])\n",
    "#    print(\"------------------------------\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Classification Metrics\n",
    "    # -----------------------------\n",
    "    fpr[name], tpr[name], roc_auc[name], inv_fpr_at_tpr[name] = {}, {}, {}, {}\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        fpr[name][c], tpr[name][c], _ = roc_curve(y_test_bin[:, c], cls_pred[:, c])\n",
    "        roc_auc[name][c] = auc(fpr[name][c], tpr[name][c])\n",
    "\n",
    "        # compute 1/FPR @ TPR ≈ 0.8\n",
    "        idx = np.argmin(np.abs(tpr[name][c] - target_tpr))\n",
    "        fpr_val = fpr[name][c][idx]\n",
    "        inv_fpr_at_tpr[name][c] = np.inf if fpr_val == 0 else 1.0 / fpr_val\n",
    "\n",
    "    # -----------------------------\n",
    "    # Regression Metrics\n",
    "    # -----------------------------\n",
    "    mse  = mean_squared_error(y_test_pt, pt_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = mean_absolute_error(y_test_pt, pt_pred)\n",
    "    r2   = r2_score(y_test_pt, pt_pred)\n",
    "\n",
    "    regression_metrics[name] = {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"r2\": r2,\n",
    "    }\n",
    "\n",
    "    # store summary\n",
    "    results[name] = {\n",
    "        \"roc_auc\": roc_auc[name],\n",
    "        \"inv_fpr_at_tpr\": inv_fpr_at_tpr[name],\n",
    "        \"regression\": regression_metrics[name],\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Plot ROC curves\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "for c in range(n_classes):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    for name in trained_models.keys():\n",
    "        plt.plot(\n",
    "            fpr[name][c], tpr[name][c], lw=2,\n",
    "            label=f\"{name} (AUC={roc_auc[name][c]:.3f})\"\n",
    "        )\n",
    "\n",
    "    plt.plot([0,1], [0,1], \"k--\", lw=1)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve — {class_names[c]}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(ls=\"--\", alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Plot background rejection (1/FPR @ TPR=0.8)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "for c in range(n_classes):\n",
    "    plt.figure(figsize=(7,5))\n",
    "\n",
    "    model_names = list(trained_models.keys())\n",
    "    values = [inv_fpr_at_tpr[m][c] for m in model_names]\n",
    "\n",
    "    plt.bar(model_names, values, alpha=0.8, color='royalblue')\n",
    "    plt.ylabel(r\"$1/\\mathrm{FPR}$ @ TPR=0.8\")\n",
    "    plt.title(f\"Background Rejection — {class_names[c]}\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', ls='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Plot regression: True Pt vs Predicted Pt\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    _, pt_pred = model.predict(X_test, verbose=0)\n",
    "    pt_pred = pt_pred.reshape(-1)\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(y_test_pt, pt_pred, s=3, alpha=0.3)\n",
    "    plt.xlabel(\"True Jet $p_T$\")\n",
    "    plt.ylabel(\"Predicted Jet $p_T$\")\n",
    "    plt.title(f\"Regression Scatter Plot — {name}\")\n",
    "    plt.grid(ls=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5. Plot regression residuals\n",
    "# ---------------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5. Plot regression residuals with mean & RMS in legend/text\n",
    "# ---------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"Plotting residuals for {name}...\")\n",
    "    \n",
    "    _, pt_pred = model.predict(X_test, verbose=0)\n",
    "    pt_pred = pt_pred.reshape(-1)\n",
    "    pt_true = y_reg_test.flatten()\n",
    "    \n",
    "    residuals = pt_true - pt_pred\n",
    "    \n",
    "    # Compute statistics\n",
    "    mean_res = np.mean(residuals)\n",
    "    std_res  = np.std(residuals)\n",
    "    rms_res  = np.sqrt(np.mean(residuals**2))  # sometimes preferred over std\n",
    "    \n",
    "    plt.figure(figsize=(8, 5.5))\n",
    "    \n",
    "    # Histogram\n",
    "    counts, bins, _ = plt.hist(residuals, bins=100, range=(-500, 500), alpha=0.75, color='steelblue',\n",
    "                               edgecolor='black', linewidth=0.5, density=False)\n",
    "    \n",
    "    # Optional: overplot a nice label box\n",
    "    textstr = '\\n'.join([\n",
    "        rf'Mean (bias) = ${mean_res:+.2f}\\ \\mathrm{{GeV}}$',\n",
    "        rf'Std. dev.  = ${std_res:.2f}\\ \\mathrm{{GeV}}$',\n",
    "        rf'RMS         = ${rms_res:.2f}\\ \\mathrm{{GeV}}$',\n",
    "        rf'Entries     = {len(residuals):,}'\n",
    "    ])\n",
    "    \n",
    "    # Place a beautiful box in the upper right (adjust loc if needed)\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.85, edgecolor='gray')\n",
    "    plt.text(0.95, 0.95, textstr, transform=plt.gca().transAxes,\n",
    "             fontsize=11, verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=props)\n",
    "\n",
    "    plt.xlabel(r'Residual  $(p_T^\\mathrm{true} - p_T^\\mathrm{pred})$ [GeV]', fontsize=13)\n",
    "    plt.ylabel('Jets', fontsize=13)\n",
    "    plt.title(f'Regression Residuals — {name}', fontsize=14, pad=15)\n",
    "    plt.grid(True, alpha=0.3, ls='--')\n",
    "    plt.xlim(-500, 500)   # adjust if your residuals are wider\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Also print to console\n",
    "    print(f\"{name} → Bias: {mean_res:+.3f} GeV | Std: {std_res:.3f} GeV | RMS: {rms_res:.3f} GeV\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b3ff8-2200-411a-b4c9-fd43e2719616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow as tf\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Settings\n",
    "# ------------------------------------------------------------------\n",
    "target_tpr = 0.80                         # we want 1/FPR @ 80% signal efficiency\n",
    "class_names = ['Top', 'HQQ', 'HTauTau', 'Wqq', 'QCD']\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "\n",
    "# Pre-compute integer labels once (faster + cleaner)\n",
    "y_test_int = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Containers\n",
    "fpr_dict      = {}\n",
    "tpr_dict      = {}\n",
    "auc_dict      = {}\n",
    "rejection_dict = {}   # this will hold 1/FPR @ TPR=0.8\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Loop over all trained models\n",
    "# ------------------------------------------------------------------\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"Evaluating ROC for model: {name}\")\n",
    "\n",
    "    # Predict logits → convert to probabilities (CRITICAL!)\n",
    "    cls_logits, _ = model.predict(X_test, verbose=0)\n",
    "    cls_proba = tf.nn.softmax(cls_logits, axis=-1).numpy()\n",
    "\n",
    "    fpr_dict[name]      = {}\n",
    "    tpr_dict[name]      = {}\n",
    "    auc_dict[name]      = {}\n",
    "    rejection_dict[name] = {}\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        # BEST WAY: binary label + probability of class c\n",
    "        fpr, tpr, _ = roc_curve(y_test_int == c, cls_proba[:, c])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Store\n",
    "        fpr_dict[name][c] = fpr\n",
    "        tpr_dict[name][c] = tpr\n",
    "        auc_dict[name][c] = roc_auc\n",
    "\n",
    "        # -------------------------------\n",
    "        # 1/FPR @ exactly TPR = 0.80\n",
    "        # -------------------------------\n",
    "        # Find the point closest to 80%\n",
    "        idx = np.argmin(np.abs(tpr - target_tpr))\n",
    "\n",
    "        # Interpolate for more precision (optional but nicer)\n",
    "        if idx > 0 and idx < len(tpr)-1:\n",
    "            tpr_low  = tpr[idx-1]\n",
    "            tpr_high = tpr[idx]\n",
    "            fpr_low  = fpr[idx-1]\n",
    "            fpr_high = fpr[idx]\n",
    "\n",
    "            if tpr_high != tpr_low:\n",
    "                # Linear interpolation in log(FPR) space\n",
    "                alpha = (target_tpr - tpr_low) / (tpr_high - tpr_low)\n",
    "                fpr_at_80 = np.exp(np.log(fpr_low) * (1-alpha) + np.log(fpr_high) * alpha)\n",
    "            else:\n",
    "                fpr_at_80 = fpr[idx]\n",
    "        else:\n",
    "            fpr_at_80 = fpr[idx]\n",
    "\n",
    "        # Safe handling of FPR = 0 → infinite rejection\n",
    "        if fpr_at_80 <= 0 or np.isnan(fpr_at_80):\n",
    "            rejection = 1e6   # visual cap (you can change to any large number)\n",
    "        else:\n",
    "            rejection = 1.0 / fpr_at_80\n",
    "\n",
    "        rejection_dict[name][c] = rejection\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Plot Background Rejection (1/FPR @ TPR=0.8) per class\n",
    "# ------------------------------------------------------------------\n",
    "model_names = list(trained_models.keys())\n",
    "\n",
    "for c in range(n_classes):\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    \n",
    "    values = [rejection_dict[model][c] for model in model_names]\n",
    "    \n",
    "    # Cap very large values for nice plotting (optional)\n",
    "    capped_values = [min(v, 15000) if not np.isinf(v) else 15000 for v in values]\n",
    "    \n",
    "    bars = plt.bar(model_names, capped_values, color=colors[c], alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    # Write real value on top of each bar\n",
    "    for bar, real_val in zip(bars, values):\n",
    "        if real_val >= 15000:\n",
    "            txt = \"∞\" if np.isinf(real_val) else f\"{real_val:,.0f}\"\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 300,\n",
    "                     txt, ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "        else:\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100,\n",
    "                     f\"{real_val:,.0f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.ylabel(r\"Background Rejection = $1/\\mathrm{FPR}$ @ TPR = 80%\", fontsize=13)\n",
    "    plt.title(f\"Background Rejection — {class_names[c]}\", fontsize=15, pad=20)\n",
    "    plt.ylim(0, 16000)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3, ls='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3b808-2538-4667-8491-224fc4a551ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Plot helper\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def plot_history(history, metrics, ylabel=\"Value\", title=None, logy=False):\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    for metric in metrics:\n",
    "        if metric in history.history:\n",
    "            ax.plot(history.history[metric], label=metric)\n",
    "        else:\n",
    "            print(f\"[WARN] Metric {metric} not found in history.\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    if logy:\n",
    "        ax.set_yscale(\"log\")\n",
    "    ax.legend()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Unified TEST function (classification + regression)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def test_model(model, save_path: Path, X_train, X_test, Y_test_cls, Y_test_pt):\n",
    "    \"\"\"\n",
    "    Evaluates a multi-task model with:\n",
    "      - classification head: cls_output\n",
    "      - regression head: pt_output\n",
    "    Computes several metrics and saves JSON.\n",
    "\n",
    "    Arguments:\n",
    "      X_train: needed for trace_minmax\n",
    "      Y_test_cls: integer labels for classification\n",
    "      Y_test_pt: true jet Pt for regression\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "    save_path = Path(save_path)\n",
    "    (save_path / \"models\").mkdir(exist_ok=True)\n",
    "\n",
    "    ckpts = list(save_path.glob(\"ckpts/*.keras\"))\n",
    "    pbar = tqdm(ckpts)\n",
    "\n",
    "    X_train = np.array(X_train, np.float32)\n",
    "    X_test  = np.array(X_test,  np.float32)\n",
    "\n",
    "    Y_test_cls = np.array(Y_test_cls)\n",
    "    Y_test_pt  = np.array(Y_test_pt).reshape(-1)\n",
    "\n",
    "    for ckpt in pbar:\n",
    "        model.load_weights(ckpt)\n",
    "\n",
    "        # Optional: normalize activations\n",
    "        # trace_minmax(model, X_train, batch_size=16384)\n",
    "\n",
    "        # Predictions\n",
    "        cls_pred, pt_pred = model.predict(X_test, batch_size=16384, verbose=0)\n",
    "        cls_pred_labels = np.argmax(cls_pred, axis=1)\n",
    "        pt_pred = pt_pred.reshape(-1)\n",
    "\n",
    "        # Compute classification\n",
    "        cls_acc = np.mean(cls_pred_labels == Y_test_cls)\n",
    "\n",
    "        # Regression metrics\n",
    "        mse  = mean_squared_error(Y_test_pt, pt_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae  = mean_absolute_error(Y_test_pt, pt_pred)\n",
    "        r2   = r2_score(Y_test_pt, pt_pred)\n",
    "\n",
    "        # EBOPs\n",
    "        ebops = sum(float(layer.ebops) for layer in model.layers if hasattr(layer, \"ebops\"))\n",
    "\n",
    "        # Store\n",
    "        results[ckpt.name] = {\n",
    "            \"classification_accuracy\": float(cls_acc),\n",
    "            \"regression_mse\": float(mse),\n",
    "            \"regression_rmse\": float(rmse),\n",
    "            \"regression_mae\": float(mae),\n",
    "            \"regression_r2\": float(r2),\n",
    "            \"ebops\": ebops,\n",
    "        }\n",
    "\n",
    "        # Save a copy of the model with these weights\n",
    "        model.save(save_path / \"models\" / f\"{ckpt.stem}.keras\")\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Acc: {cls_acc:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f} @ {ebops:.0f} EBOPs\"\n",
    "        )\n",
    "\n",
    "    # Save all results\n",
    "    with open(save_path / \"test_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Run test + generate plots\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(\"Running unified evaluation...\")\n",
    "\n",
    "results = test_model(\n",
    "    model=model,\n",
    "    save_path=Path(save_path),\n",
    "    X_train=X_train_val,\n",
    "    X_test=X_test,\n",
    "    Y_test_cls=y_test,        # classification labels\n",
    "    Y_test_pt=y_reg_test      # regression true Pt values\n",
    ")\n",
    "\n",
    "print(\"Saved test metrics to: {}/test_results.json\".format(save_path))\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. PLOTS\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# --- Total + component losses\n",
    "plot_history(\n",
    "    history,\n",
    "    metrics=[\n",
    "        \"loss\", \"val_loss\",\n",
    "        \"cls_output_loss\", \"val_cls_output_loss\",\n",
    "        \"pt_output_loss\",  \"val_pt_output_loss\",\n",
    "    ],\n",
    "    ylabel=\"Loss\",\n",
    "    title=\"Loss Curves\",\n",
    "    logy=True,\n",
    ");\n",
    "\n",
    "# --- Classification accuracy\n",
    "plot_history(\n",
    "    history,\n",
    "    metrics=[\n",
    "        \"cls_output_accuracy\",\n",
    "        \"val_cls_output_accuracy\"\n",
    "    ],\n",
    "    ylabel=\"Accuracy\",\n",
    "    title=\"Classification Accuracy\",\n",
    ");\n",
    "\n",
    "# --- Regression MSE\n",
    "plot_history(\n",
    "    history,\n",
    "    metrics=[\n",
    "        \"pt_output_mse\",\n",
    "        \"val_pt_output_mse\"\n",
    "    ],\n",
    "    ylabel=\"MSE\",\n",
    "    title=\"Regression MSE\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9b949-3254-4ae2-94d3-1e2adb79ce41",
   "metadata": {},
   "source": [
    "### Feature importance based on SHAP, Permutation and Gradient Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfda4a-1955-4c24-8004-c5da52a0bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "from pathlib import Path\n",
    "\n",
    "save_path = Path(work_path)\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "'''\n",
    "# ------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------------------------------------\n",
    "def permutation_importance(model, X, y, metric=log_loss):\n",
    "    \"\"\"Model-agnostic permutation importance.\"\"\"\n",
    "    baseline = metric(y, model.predict(X))\n",
    "    importances = []\n",
    "    for i in range(X.shape[1]):\n",
    "        Xp = X.copy()\n",
    "        np.random.shuffle(Xp[:, i])\n",
    "        score = metric(y, model.predict(Xp))\n",
    "        importances.append(score - baseline)\n",
    "    return np.array(importances)\n",
    "\n",
    "def gradient_importance(model, X):\n",
    "    \"\"\"Mean absolute gradient of output w.r.t. input.\"\"\"\n",
    "    X_tf = tf.convert_to_tensor(X)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(X_tf)\n",
    "        preds = model(X_tf)\n",
    "    grads = tape.gradient(preds, X_tf)\n",
    "    return tf.reduce_mean(tf.abs(grads), axis=0).numpy()\n",
    "\n",
    "def plot_importances(importances, feature_names, title, savefile):\n",
    "    \"\"\"Generic plotting function.\"\"\"\n",
    "    sorted_idx = np.argsort(importances)[::-1]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(range(len(importances)), importances[sorted_idx])\n",
    "    plt.xticks(range(len(importances)), np.array(feature_names)[sorted_idx],\n",
    "               rotation=90, fontsize=8)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savefile, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SHAP background data\n",
    "# ------------------------------------------------------------\n",
    "background = X_test[np.random.choice(X_test.shape[0], 100, replace=False)]\n",
    "X_explain = X_test[:200]\n",
    "\n",
    "# If you have feature names, define them; otherwise, use indices\n",
    "try:\n",
    "    feature_names = X_test.columns\n",
    "except Exception:\n",
    "    feature_names = [f\"feat_{i}\" for i in range(X_test.shape[1])]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Loop over trained models\n",
    "# ------------------------------------------------------------\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"\\n=== Evaluating feature importance for {name} ===\")\n",
    "\n",
    "    # --- SHAP ---\n",
    "    try:\n",
    "        print(\" → Computing SHAP values...\")\n",
    "        explainer = shap.DeepExplainer(model, background)\n",
    "        shap_values = explainer.shap_values(X_explain)\n",
    "        shap.summary_plot(shap_values, X_explain, show=False)\n",
    "        plt.savefig(save_path / f'shap_summary_{name}.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        np.save(save_path / f'shap_values_{name}.npy', shap_values)\n",
    "        print(\"   ✓ SHAP values saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ SHAP failed for {name}: {e}\")\n",
    "\n",
    "    # --- Permutation Importance ---\n",
    "    try:\n",
    "        print(\" → Computing permutation importances...\")\n",
    "        importances = permutation_importance(model, X_test, y_test)\n",
    "        np.save(save_path / f'perm_importance_{name}.npy', importances)\n",
    "\n",
    "        # Plot permutation importances\n",
    "        plot_importances(importances, feature_names,\n",
    "                         f\"Permutation Importance — {name}\",\n",
    "                         save_path / f'perm_importance_{name}.png')\n",
    "        print(\"   ✓ Permutation importances saved and plotted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Permutation importance failed for {name}: {e}\")\n",
    "\n",
    "    # --- Gradient-based Importance ---\n",
    "    try:\n",
    "        print(\" → Computing gradient-based importances...\")\n",
    "        grads = gradient_importance(model, X_test[:500])  # subset for speed\n",
    "        np.save(save_path / f'gradients_{name}.npy', grads)\n",
    "\n",
    "        # Plot gradient importances\n",
    "        plot_importances(grads, feature_names,\n",
    "                         f\"Gradient-based Importance — {name}\",\n",
    "                         save_path / f'gradients_{name}.png')\n",
    "        print(\"   ✓ Gradient importances saved and plotted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Gradient-based importance failed for {name}: {e}\")\n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------------------------------------\n",
    "def permutation_importance(model, X, y, metric=log_loss):\n",
    "    \"\"\"Model-agnostic permutation importance.\"\"\"\n",
    "    baseline = metric(y, model.predict(X))\n",
    "    importances = []\n",
    "    for i in range(X.shape[1]):\n",
    "        Xp = X.copy()\n",
    "        np.random.shuffle(Xp[:, i])\n",
    "        score = metric(y, model.predict(Xp))\n",
    "        importances.append(score - baseline)\n",
    "    return np.array(importances)\n",
    "\n",
    "def gradient_importance(model, X):\n",
    "    \"\"\"Mean absolute gradient of output w.r.t. input.\"\"\"\n",
    "    X_tf = tf.convert_to_tensor(X)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(X_tf)\n",
    "        preds = model(X_tf)\n",
    "    grads = tape.gradient(preds, X_tf)\n",
    "    return tf.reduce_mean(tf.abs(grads), axis=0).numpy()\n",
    "\n",
    "def plot_importances_inline(importances, feature_names, title, ax):\n",
    "    \"\"\"Generic plotting function for inline plotting.\"\"\"\n",
    "    sorted_idx = np.argsort(importances)[::-1]\n",
    "    ax.bar(range(len(importances)), importances[sorted_idx])\n",
    "    ax.set_xticks(range(len(importances)))\n",
    "    ax.set_xticklabels(np.array(feature_names)[sorted_idx], rotation=90, fontsize=8)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SHAP background and feature names\n",
    "# ------------------------------------------------------------\n",
    "background = X_test[np.random.choice(X_test.shape[0], 100, replace=False)]\n",
    "X_explain = X_test[:200]\n",
    "\n",
    "try:\n",
    "    feature_names = X_test.columns\n",
    "except Exception:\n",
    "    feature_names = [f\"feat_{i}\" for i in range(X_test.shape[1])]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Loop over trained models\n",
    "# ------------------------------------------------------------\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"\\n=== Feature importance for model: {name} ===\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1) SHAP\n",
    "    # ------------------------------------------------------------\n",
    "    try:\n",
    "        print(\" → SHAP values...\")\n",
    "        explainer = shap.DeepExplainer(model, background)\n",
    "        shap_values = explainer.shap_values(X_explain)\n",
    "\n",
    "        # Display summary in notebook\n",
    "        shap.summary_plot(shap_values, X_explain, feature_names=feature_names, show=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ SHAP failed: {e}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Prepare 6 side-by-side axes (3 per row, 2 rows)\n",
    "    # ------------------------------------------------------------\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    plot_idx = 0\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2) Permutation Importance\n",
    "    # ------------------------------------------------------------\n",
    "    try:\n",
    "        print(\" → Permutation importance...\")\n",
    "        perm_imp = permutation_importance(model, X_test, y_test)\n",
    "        plot_importances_inline(perm_imp, feature_names,\n",
    "                                f\"Permutation – {name}\", axes[plot_idx])\n",
    "        plot_idx += 1\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Permutation failed: {e}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3) Gradient Importance\n",
    "    # ------------------------------------------------------------\n",
    "    try:\n",
    "        print(\" → Gradient importance...\")\n",
    "        grads = gradient_importance(model, X_test[:500])\n",
    "        plot_importances_inline(grads, feature_names,\n",
    "                                f\"Gradient – {name}\", axes[plot_idx])\n",
    "        plot_idx += 1\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Gradient failed: {e}\")\n",
    "\n",
    "    # Format spare plots cleanly\n",
    "    for k in range(plot_idx, len(axes)):\n",
    "        axes[k].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f21fe7-312a-4806-9270-5ba61ada8f43",
   "metadata": {},
   "source": [
    "## Plot Features for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2dbad-f5bd-442e-b944-9a03ca6f5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "CLASSES = list(class_labels.keys())       # [\"Top\",\"HQQ\",\"HTauTau\",\"Wqq\",\"QCD\"]\n",
    "colors  = {\"Top\":\"red\", \"HQQ\":\"blue\", \"HTauTau\":\"purple\", \"Wqq\":\"green\", \"QCD\":\"orange\"}\n",
    "ncols = 3  # number of plots per row\n",
    "\n",
    "# -------------------------------\n",
    "# Constituent features (step histograms)\n",
    "# -------------------------------\n",
    "n_features = len(const_feature_names)\n",
    "nrows = int(np.ceil(n_features / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 4*nrows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ifeat, featname in enumerate(const_feature_names):\n",
    "    ax = axes[ifeat]\n",
    "    \n",
    "    for cls in CLASSES:\n",
    "        cls_label_idx = label_to_idx[class_labels[cls]]\n",
    "        mask = labels_array == cls_label_idx\n",
    "        \n",
    "        Xc = X_constituents[mask, :, ifeat].flatten()\n",
    "        \n",
    "        ax.hist(\n",
    "            Xc,\n",
    "            bins=60,\n",
    "            density=True,\n",
    "            histtype='step',        # contour only\n",
    "            linewidth=1.5,\n",
    "            label=cls,\n",
    "            color=colors[cls]\n",
    "        )\n",
    "    \n",
    "    ax.set_title(featname)\n",
    "    ax.set_xlabel(featname)\n",
    "    ax.set_ylabel(\"Normalized entries\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "# remove empty axes\n",
    "for i in range(n_features, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Jet-level features (step histograms)\n",
    "# -------------------------------\n",
    "n_features = len(jet_feature_names)\n",
    "nrows = int(np.ceil(n_features / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 4*nrows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ifeat, featname in enumerate(jet_feature_names):\n",
    "    ax = axes[ifeat]\n",
    "    \n",
    "    for cls in CLASSES:\n",
    "        cls_label_idx = label_to_idx[class_labels[cls]]\n",
    "        mask = labels_array == cls_label_idx\n",
    "        \n",
    "        Xj = X_jets[mask, ifeat]\n",
    "        \n",
    "        ax.hist(\n",
    "            Xj,\n",
    "            bins=60,\n",
    "            density=True,\n",
    "            histtype='step',        # contour only\n",
    "            linewidth=1.5,\n",
    "            label=cls,\n",
    "            color=colors[cls]\n",
    "        )\n",
    "    \n",
    "    ax.set_title(featname)\n",
    "    ax.set_xlabel(featname)\n",
    "    ax.set_ylabel(\"Normalized entries\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "# remove empty axes\n",
    "for i in range(n_features, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f72553-b55f-40e5-9573-d9ae256a6816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699ebba8-9724-4174-b903-eda8b7de0831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352d631-e8b6-4f55-964e-9da1f2513963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
